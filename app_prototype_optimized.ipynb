{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language Learning App Prototype Notebook\n",
        "\n",
        "Welcome to the Language Learning App Prototype! This notebook demonstrates an initial prototype for a language learning application. The app predicts and translates words that a learner might find challenging based on their proficiency level and the words they mark as unknown.\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "1. **Run All Cells**: To get started, you need to run all the cells in this notebook. This will install the necessary libraries, download required data, and set up the environment.\n",
        "\n",
        "2. **Interface Deployment**: Once all cells are run, a Gradio interface will be deployed at the bottom of this notebook. You will interact with this interface to use the app.\n",
        "\n",
        "3. **Using the Interface**:\n",
        "   - **Native Language**: Select your native language (currently only English is available).\n",
        "   - **Target Language**: Select the target language you want to learn (currently only Spanish is available).\n",
        "   - **Proficiency Level**: Choose your proficiency level from A1 to C2.\n",
        "   - **Text Input**: Enter the text you want to read and learn from in the target language.\n",
        "   - **Start**: Click the 'Start' button to begin the process. The app will process the text, predict unknown words, and provide translations.\n",
        "   - **Input Unknown Words**: You can input any additional unknown words you encounter.\n",
        "   - **Next Paragraph**: Click the 'Next Paragraph' button to process the next paragraph of text.\n",
        "   - **Restart**: If you want to start over, click the 'Restart' button to reset the interface.\n"
      ],
      "metadata": {
        "id": "9eoNE5YgLc3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Installation\n",
        "\n",
        "In this section, we install all the necessary libraries required for our language learning application. These libraries include NLP tools, translation services, frequency analysis tools, and the Gradio library for building the user interface."
      ],
      "metadata": {
        "id": "ik7j9REClybR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o_qyrBCKgKR"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install stanza\n",
        "!pip install -U deep-translator\n",
        "!pip install langdetect\n",
        "!pip install wordfreq\n",
        "!pip install wiktionaryparser\n",
        "!pip install nltk\n",
        "!pip install gradio\n",
        "!pip install rapidfuzz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Download\n",
        "\n",
        "Here we download and extract the CogNet data, which contains cognate pairs between English and Spanish. This data is essential for identifying cognates in the text and providing accurate translations.\n"
      ],
      "metadata": {
        "id": "zFiX7B2XmMId"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NEN-SF3DvqM"
      },
      "outputs": [],
      "source": [
        "# Download and extract the CogNet data\n",
        "!wget https://github.com/kbatsuren/CogNet/raw/master/CogNet-v2.0.zip\n",
        "!unzip CogNet-v2.0.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Imports\n",
        "\n",
        "This section imports all the necessary libraries that we will use throughout the notebook. These libraries provide functionalities such as natural language processing, translation, and data manipulation.\n"
      ],
      "metadata": {
        "id": "Fnrn15Q9mchs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cProfile\n",
        "import pstats\n",
        "import io\n",
        "import gradio as gr\n",
        "import stanza\n",
        "from deep_translator import GoogleTranslator\n",
        "from collections import defaultdict\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from wordfreq import word_frequency\n",
        "from rapidfuzz import fuzz, process\n",
        "import pandas as pd\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "MFmC0P7rmd3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stanza Pipeline Initialization\n",
        "\n",
        "Here we download the necessary language models for Stanza and initialize the Stanza pipelines for English and Spanish. These pipelines will be used for tokenization, lemmatization, part-of-speech tagging, and named entity recognition.\n"
      ],
      "metadata": {
        "id": "mwoBHTPAmn1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Stanza pipelines (only once) with specific components\n",
        "stanza.download('en')\n",
        "stanza.download('es')\n",
        "nlp_native = stanza.Pipeline('en', processors='tokenize,lemma,pos')\n",
        "nlp_target = stanza.Pipeline('es', processors='tokenize,lemma,pos,ner')"
      ],
      "metadata": {
        "id": "caxToWV9msLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Snowball Stemmer Initialization\n",
        "\n",
        "This section initializes the Snowball Stemmer for the Spanish language. The stemmer will be used to reduce words to their root forms, which helps in identifying morphological similarities between words."
      ],
      "metadata": {
        "id": "M2ZDxNE5m3wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Snowball Stemmer\n",
        "stemmer = SnowballStemmer(\"spanish\")"
      ],
      "metadata": {
        "id": "Qbawc1Qem8ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Filtering\n",
        "\n",
        "In this section, we load the CogNet data into a DataFrame and filter it to get the Spanish-English cognates. This filtered data will be used to identify cognates in the input text.\n"
      ],
      "metadata": {
        "id": "f7A-PHUgnCzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TSV file into a DataFrame, skipping bad lines and using the Python engine\n",
        "cognet_df = pd.read_csv('CogNet-v2.0.tsv', sep='\\t', header=None,\n",
        "                        names=['concept_id', 'lang1', 'word1', 'lang2', 'word2', 'translit1', 'translit2'],\n",
        "                        on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Filter for Spanish-English cognates\n",
        "cognet_sp_en = cognet_df[((cognet_df['lang1'] == 'spa') & (cognet_df['lang2'] == 'eng')) |\n",
        "                         ((cognet_df['lang1'] == 'eng') & (cognet_df['lang2'] == 'spa'))]\n"
      ],
      "metadata": {
        "id": "qHRi4gvdBTEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Variables\n",
        "\n",
        "This section initializes various variables and data structures that will hold the state of the application, such as paragraphs, known words, unknown words, and translations.\n"
      ],
      "metadata": {
        "id": "tx2Qpp8KnSMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_thresholds = {\n",
        "    'A1': 0.0001,\n",
        "    'A2': 0.00001,\n",
        "    'B1': 0.000001,\n",
        "    'B2': 0.0000005,\n",
        "    'C1': 0.0000001,\n",
        "    'C2': 0.00000005\n",
        "}\n",
        "\n",
        "translation_cache = {}\n",
        "\n",
        "def initialize_variables():\n",
        "    global state\n",
        "    state = {\n",
        "        'paragraphs': [],\n",
        "        'current_paragraph_index': 0,\n",
        "        'known_words': [],\n",
        "        'unknown_words': [],\n",
        "        'validated_translations': [],\n",
        "        'word_count': defaultdict(int),\n",
        "        'all_final_unknown_words': [],\n",
        "        'all_cognate_pairs': {},\n",
        "        'final_unknown_words_dict': defaultdict(set),\n",
        "        'original_word_mapping': {},\n",
        "        'native_language': '',\n",
        "        'target_language': '',\n",
        "        'level': '',\n",
        "        'final_unknown_word_counts': defaultdict(int),\n",
        "        'nlp_cache': {},\n",
        "        'frequency_cache': {},\n",
        "        'ner_cache': {},\n",
        "        'merged_paragraphs': []\n",
        "    }\n",
        "\n",
        "# Initialize state for the first time\n",
        "initialize_variables()"
      ],
      "metadata": {
        "id": "2hEnKsDNnUEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cognate Identification Function\n",
        "\n",
        "This function identifies cognates between Spanish and English words using the CogNet data and independent similarity checks. Cognates are words in two languages that have a common etymological origin.\n"
      ],
      "metadata": {
        "id": "PRLMZpnGne6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to identify cognates\n",
        "def find_cognates(spanish_words, english_words, cognet_df, similarity_threshold=65):\n",
        "    cognates = []\n",
        "    spanish_words_lower = [sp_word.lower() for sp_word in spanish_words]\n",
        "    english_words_lower = [en_word.lower() for en_word in english_words]\n",
        "\n",
        "    # Check for cognates in CogNet\n",
        "    for sp_word in spanish_words_lower:\n",
        "        matches = cognet_df[(cognet_df['word1'].str.lower() == sp_word) | (cognet_df['word2'].str.lower() == sp_word)]\n",
        "        for index, row in matches.iterrows():\n",
        "            if row['lang1'] == 'spa' and row['lang2'] == 'eng' and row['word2'].lower() in english_words_lower:\n",
        "                en_word = row['word2']\n",
        "            elif row['lang1'] == 'eng' and row['lang2'] == 'spa' and row['word1'].lower() in english_words_lower:\n",
        "                en_word = row['word1']\n",
        "            else:\n",
        "                continue\n",
        "            similarity = fuzz.ratio(sp_word, en_word.lower())\n",
        "            if similarity >= similarity_threshold:\n",
        "                cognates.append((sp_word, en_word))\n",
        "\n",
        "    # Indentify lemma-based cognates\n",
        "    for sp_word in spanish_words:\n",
        "        sp_features = state['nlp_cache'].get(sp_word, {})\n",
        "        for en_word in english_words:\n",
        "            en_features = state['nlp_cache'].get(en_word, {})\n",
        "            # Check similarity\n",
        "            similarity = fuzz.ratio(sp_word.lower(), en_word.lower())\n",
        "            if similarity >= similarity_threshold:\n",
        "                cognates.append((sp_word, en_word))\n",
        "            elif sp_features and en_features and sp_features['lemma'] == en_features['lemma']:\n",
        "                cognates.append((sp_word, en_word))\n",
        "\n",
        "    return cognates"
      ],
      "metadata": {
        "id": "VPOXtpPRn8U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Safe Translation Function\n",
        "\n",
        "This section defines the `safe_translate` function, which translates sentences with retry logic to handle errors. The function attempts to translate each sentence up to three times before returning a fallback message. This ensures that temporary issues with the translation service do not cause the app to fail."
      ],
      "metadata": {
        "id": "_cOL9b88odAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safely translate a sentence\n",
        "def safe_translate(sentences, src, dest, retries=3):\n",
        "    translations = []\n",
        "    for sentence in sentences:\n",
        "        if sentence in translation_cache:\n",
        "            translations.append(translation_cache[sentence])\n",
        "        else:\n",
        "            for _ in range(retries):\n",
        "                try:\n",
        "                    translation = GoogleTranslator(source=src, target=dest).translate(sentence)\n",
        "                    if translation:\n",
        "                        translations.append(translation)\n",
        "                        translation_cache[sentence] = translation\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error translating sentence '{sentence}': {e}\")\n",
        "                    time.sleep(1)\n",
        "            else:\n",
        "                translations.append(\"Translation not available\")\n",
        "    return translations"
      ],
      "metadata": {
        "id": "1MZ6nLUIoiDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Translation Function\n",
        "\n",
        "Here we define the `batch_translate` function, which translates a list of sentences in batch mode. This function uses the GoogleTranslator to translate sentences from the target language to the native language. It also caches translations to avoid redundant API calls, improving performance.\n"
      ],
      "metadata": {
        "id": "aPG8D2VDoxVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch translation\n",
        "def batch_translate(sentences, src, dest):\n",
        "    translations = []\n",
        "    for sentence in sentences:\n",
        "        if sentence in translation_cache:\n",
        "            translations.append(translation_cache[sentence])\n",
        "        else:\n",
        "            try:\n",
        "                translation = GoogleTranslator(source=src, target=dest).translate(sentence)\n",
        "                if translation:\n",
        "                    translations.append(translation)\n",
        "                    translation_cache[sentence] = translation\n",
        "                else:\n",
        "                    translations.append(\"Translation not available\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error translating sentence '{sentence}': {e}\")\n",
        "                translations.append(\"Translation not available\")\n",
        "    return translations"
      ],
      "metadata": {
        "id": "Re7OhArMo08h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Morphological Similarity Check Function\n",
        "\n",
        "This section introduces the `is_similar_morphology` function. The function checks if two words are morphologically similar based on their stems and lemmas. This is useful for identifying words that are related or share a common root, which can help in predicting unknown words."
      ],
      "metadata": {
        "id": "k1Yh_b2lpD-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check morphological similarity\n",
        "def is_similar_morphology(word1, word2, threshold):\n",
        "    if word1['stem'] == word2['stem']:\n",
        "        return True\n",
        "    if (word1['stem'] in word2['stem'] or word2['stem'] in word1['stem']) and word2['frequency'] < threshold:\n",
        "        return True\n",
        "    if word1['lemma'] == word2['lemma']:\n",
        "        return True\n",
        "    if (word1['lemma'] in word2['lemma'] or word2['lemma'] in word1['lemma']) and word2['frequency'] < threshold:\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "2AXUX_-EpGpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing Function\n",
        "\n",
        "In this section, we define the `batch_preprocess_text` function. This function tokenizes, lemmatizes, and performs part-of-speech tagging on the input text using Stanza. It also calculates word frequencies and caches the results to improve performance. The preprocessing steps are essential for understanding the structure of the text and identifying which words might be challenging for learners.\n"
      ],
      "metadata": {
        "id": "XUYLp3FIpMKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text (tokenize, lemmatize, POS tagging)\n",
        "def batch_preprocess_text(paragraphs, nlp, target_language, use_cache=True):\n",
        "    batch_text = \"\\n\\n\".join(paragraphs)\n",
        "    if use_cache and batch_text in state['nlp_cache']:\n",
        "        return state['nlp_cache'][batch_text]\n",
        "\n",
        "    doc = nlp(batch_text)\n",
        "    sentences = [sentence.text for sentence in doc.sentences]\n",
        "    words = []\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            if len(word.text) > 1:  # Ensure we are processing only whole words\n",
        "                if word.text in state['frequency_cache']:\n",
        "                    frequency = state['frequency_cache'][word.text]\n",
        "                else:\n",
        "                    frequency = word_frequency(word.text, target_language)\n",
        "                    state['frequency_cache'][word.text] = frequency\n",
        "                word_features = {\n",
        "                    'text': word.text,\n",
        "                    'lemma': word.lemma,\n",
        "                    'pos': word.upos,\n",
        "                    'frequency': frequency,\n",
        "                    'stem': stemmer.stem(word.text)\n",
        "                }\n",
        "                words.append(word_features)\n",
        "                state['nlp_cache'][word.text.lower()] = word_features\n",
        "                print(f\"Added to cache: {word_features}\")  # Logging cache addition\n",
        "\n",
        "    if use_cache:\n",
        "        state['nlp_cache'][batch_text] = (sentences, words)\n",
        "    return sentences, words"
      ],
      "metadata": {
        "id": "ZlwvqmvXpn4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER) Function\n",
        "\n",
        "This section defines the `perform_ner` function, which performs named entity recognition on the input text using the Stanza pipeline. Named entities (e.g., names of people, places, organizations) are often known words for language learners, and recognizing them helps in accurately predicting unknown words."
      ],
      "metadata": {
        "id": "T4Z5uNDrppsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Named Entity Recognition (NER)\n",
        "def perform_ner(text, nlp):\n",
        "    if text in state['ner_cache']:\n",
        "        return state['ner_cache'][text]\n",
        "\n",
        "    doc = nlp(text)\n",
        "    entities = [entity.text.lower() for sentence in doc.sentences for entity in sentence.ents]\n",
        "    state['ner_cache'][text] = entities\n",
        "    return entities"
      ],
      "metadata": {
        "id": "GaxnK5WYpzkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation Validation Function\n",
        "\n",
        "This section defines the `validate_translation_in_context` function, which aims to ensure that the translations make sense within the given context. The function uses several checks:\n",
        "\n",
        "1. **Initial Check (Direct Match)**: It directly matches the translated word with words in the translated sentence.\n",
        "2. **Similarity Check**: It uses fuzzy string matching to find the most similar word in the translated sentence.\n",
        "3. **POS Tag Check**: It matches the part-of-speech (POS) tag of the translated word with words in the translated sentence.\n",
        "\n",
        "These checks help in providing accurate translations that fit well in the context of the sentences."
      ],
      "metadata": {
        "id": "h-pBN8vNp1EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate translation with context\n",
        "def validate_translation_in_context(translation, original_sentences, translated_sentences, spanish_pos):\n",
        "    for orig_sent, trans_sent in zip(original_sentences, translated_sentences):\n",
        "        if trans_sent in state['nlp_cache']:\n",
        "            doc = state['nlp_cache'][trans_sent]\n",
        "        else:\n",
        "            doc = nlp_native(trans_sent)\n",
        "            state['nlp_cache'][trans_sent] = doc\n",
        "\n",
        "        if isinstance(doc, tuple):\n",
        "            doc = doc[1]\n",
        "\n",
        "        if orig_sent in state['nlp_cache']:\n",
        "            orig_doc = state['nlp_cache'][orig_sent]\n",
        "        else:\n",
        "            orig_doc = nlp_target(orig_sent)\n",
        "            state['nlp_cache'][orig_sent] = orig_doc\n",
        "\n",
        "        if isinstance(orig_doc, tuple):\n",
        "            orig_doc = orig_doc[1]\n",
        "\n",
        "        # Initial Check: Direct match\n",
        "        for word in doc.sentences[0].words:\n",
        "            if word.text.lower() == translation.lower():\n",
        "                return word.text\n",
        "\n",
        "        # Similarity Check: Find the most similar word\n",
        "        words_in_trans_sent = [word.text for word in doc.sentences[0].words]\n",
        "        most_similar = process.extractOne(translation, words_in_trans_sent, scorer=fuzz.ratio, score_cutoff=80)\n",
        "        similar_enough = process.extractOne(translation, words_in_trans_sent, scorer=fuzz.ratio, score_cutoff=70)\n",
        "        if most_similar:\n",
        "            similar_word = most_similar[0]\n",
        "            for word in doc.sentences[0].words:\n",
        "                if word.text == similar_word:\n",
        "                    return f\"{translation}/{similar_word}\"  # Return both the individual translation and the most similar word with the same POS tag\n",
        "        if similar_enough:\n",
        "            similar_enough_word = similar_enough[0]\n",
        "            for word in doc.sentences[0].words:\n",
        "                if word.text == similar_enough_word and word.upos == spanish_pos:\n",
        "                    return f\"{translation}/{similar_enough_word}\"\n",
        "        # POS Tag Check: Find a word with the same POS tag as the Spanish word\n",
        "        pos_matches = [word.text for word in doc.sentences[0].words if word.upos == spanish_pos]\n",
        "        if len(pos_matches) == 1:\n",
        "            return f\"{translation}/{pos_matches[0]}\"  # Return both the individual translation and the POS matching word if there's only one match\n",
        "        elif len(pos_matches) > 1:\n",
        "            return translation  # Stick to the individual translation if multiple POS matches are found\n",
        "\n",
        "    # If no word with the same POS tag is found\n",
        "    return translation"
      ],
      "metadata": {
        "id": "5Z5_sN0ZqN3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profiling Decorator\n",
        "\n",
        "This section introduces a decorator function `profile_func` that profiles the execution time of functions. It helps in identifying performance bottlenecks by logging the time taken by various parts of the code, which is crucial for optimizing the app.\n"
      ],
      "metadata": {
        "id": "0KL_q0jlqPe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Profile Decorator\n",
        "def profile_func(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        pr = cProfile.Profile()\n",
        "        pr.enable()\n",
        "        result = func(*args, **kwargs)\n",
        "        pr.disable()\n",
        "        s = io.StringIO()\n",
        "        sortby = 'cumulative'\n",
        "        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
        "        ps.print_stats(10)\n",
        "        print(s.getvalue())\n",
        "        return result\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "N82pcLx3qWPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paragraph Processing Function\n",
        "\n",
        "This section defines the `process_paragraph` function, which processes each paragraph of text to predict unknown words, translate sentences, and validate translations. It involves several steps:\n",
        "\n",
        "1. **Preprocessing Text**: Tokenizes, lemmatizes, and performs POS tagging.\n",
        "2. **Named Entity Recognition**: Identifies named entities in the text.\n",
        "3. **Translation**: Translates sentences from the target language to the native language.\n",
        "4. **Identifying Cognates**: Finds cognates between the target language and the native language.\n",
        "5. **Identifying Unknown Words**: Determines which words are unknown to the learner based on frequency thresholds and similarity checks.\n",
        "\n",
        "This function combines these steps to provide a comprehensive analysis of each paragraph.\n"
      ],
      "metadata": {
        "id": "iHW9QbPIrKf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate and process each paragraph\n",
        "@profile_func\n",
        "def process_paragraph(paragraphs, input_unknown_words, known_words, unknown_words, validated_translations, word_count):\n",
        "    if not paragraphs:\n",
        "        return [], [], [], [], {}\n",
        "\n",
        "    current_paragraph_unknown_words = defaultdict(set)\n",
        "    sentences, words = batch_preprocess_text(paragraphs, nlp_target, state['target_language'])\n",
        "    entities = perform_ner(\"\\n\\n\".join(paragraphs), nlp_target)\n",
        "    translated_sentences = batch_translate(sentences, state['target_language'], state['native_language'])\n",
        "    threshold = frequency_thresholds[state['level']]\n",
        "\n",
        "    spanish_words = [word['text'].lower() for word in words]\n",
        "    english_words = batch_translate([word['text'] for word in words], state['target_language'], state['native_language'])\n",
        "    cognates = find_cognates(spanish_words, english_words, cognet_sp_en)\n",
        "    cognate_pairs = {sp: en for sp, en in cognates}\n",
        "\n",
        "    for word in words:\n",
        "        word_text = word['text'].lower()\n",
        "        state['original_word_mapping'][word_text] = word['text']\n",
        "        if word_text in entities or word['pos'] == 'PUNCT':\n",
        "            known_words.append(word)\n",
        "        elif word['frequency'] >= threshold or word_text in cognate_pairs:\n",
        "            known_words.append(word)\n",
        "        else:\n",
        "            state['final_unknown_words_dict'][word_text].add(word['lemma'])\n",
        "            current_paragraph_unknown_words[word_text].add(word['lemma'])\n",
        "\n",
        "    for word in words:\n",
        "        word_text = word['text'].lower()\n",
        "        if word_text in current_paragraph_unknown_words:\n",
        "            state['final_unknown_word_counts'][word_text] += 1\n",
        "            word_count[word_text] += 1\n",
        "\n",
        "            if state['final_unknown_word_counts'][word_text] >= 8:\n",
        "                del state['final_unknown_words_dict'][word_text]\n",
        "\n",
        "    input_unknown_word_details = []\n",
        "    for unknown_word in input_unknown_words:\n",
        "        if unknown_word:\n",
        "            processed_words = batch_preprocess_text([unknown_word], nlp_target, state['target_language'], use_cache=False)[1]\n",
        "            if processed_words:\n",
        "                processed_word = processed_words[0]\n",
        "                word_text = processed_word['text'].lower()\n",
        "                input_unknown_word_details.append(processed_word)\n",
        "                if word_text in spanish_words:\n",
        "                    state['final_unknown_words_dict'][word_text].add(processed_word['lemma'])\n",
        "                    current_paragraph_unknown_words[word_text].add(processed_word['lemma'])\n",
        "\n",
        "    for word in words:\n",
        "        for unknown_word in input_unknown_word_details:\n",
        "            if is_similar_morphology(word, unknown_word, threshold):\n",
        "                state['final_unknown_words_dict'][word['text'].lower()].add(word['lemma'])\n",
        "                current_paragraph_unknown_words[word['text'].lower()].add(word['lemma'])\n",
        "\n",
        "    for word_text, lemmas in state['final_unknown_words_dict'].items():\n",
        "        for word in words:\n",
        "            if is_similar_morphology({'text': word_text, 'lemma': next(iter(lemmas)), 'stem': stemmer.stem(word_text)}, word, threshold):\n",
        "                current_paragraph_unknown_words[word['text'].lower()].add(word['lemma'])\n",
        "\n",
        "    final_unknown_words = []\n",
        "    for word_text, lemmas in current_paragraph_unknown_words.items():\n",
        "        final_unknown_words.append({\n",
        "            'text': word_text,\n",
        "            'lemma': next(iter(lemmas)),\n",
        "            'pos': next((word['pos'] for word in words if word['text'].lower() == word_text), 'UNKNOWN'),\n",
        "            'frequency': next((word['frequency'] for word in words if word['text'].lower() == word_text), 0.0),\n",
        "            'stem': stemmer.stem(word_text)\n",
        "        })\n",
        "\n",
        "    def map_words_to_sentences(sentences, words):\n",
        "        sentence_word_map = {}\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            for word in words:\n",
        "                if word['text'].lower() in sentence.lower():\n",
        "                    sentence_word_map[word['text'].lower()] = (sentence, i)\n",
        "        return sentence_word_map\n",
        "\n",
        "    sentence_word_map = map_words_to_sentences(sentences, final_unknown_words)\n",
        "    for word in final_unknown_words:\n",
        "        if word['text'].lower() not in sentence_word_map:\n",
        "            continue\n",
        "        if word['text'] in translation_cache:\n",
        "            translation = translation_cache[word['text']]\n",
        "        else:\n",
        "            translation = GoogleTranslator(source=state['target_language'], target=state['native_language']).translate(word['text'])\n",
        "            translation_cache[word['text']] = translation\n",
        "        validated_translation = validate_translation_in_context(\n",
        "            translation,\n",
        "            sentences,\n",
        "            translated_sentences,\n",
        "            word['pos']\n",
        "        )\n",
        "        word['translation'] = validated_translation\n",
        "        word['sentence'] = sentence_word_map[word['text'].lower()][0]\n",
        "        word['translated_sentence'] = translated_sentences[sentence_word_map[word['text'].lower()][1]]\n",
        "        validated_translations.append({\n",
        "            'original': word['text'],\n",
        "            'translation': validated_translation,\n",
        "            'translated_pos': word['pos']\n",
        "        })\n",
        "\n",
        "    return sentences, translated_sentences, final_unknown_words, validated_translations, cognate_pairs"
      ],
      "metadata": {
        "id": "_Y_TFy5rrJ_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting Processing Function\n",
        "\n",
        "This section defines the `start_processing` function, which initializes the app state and begins processing the input text. The function sets the native and target languages, the proficiency level, and splits the input text into paragraphs. It then processes the first paragraph to start the app. The profiling decorator is used to measure the performance of this function.\n",
        "\n",
        "Key steps:\n",
        "1. **Initialize Variables**: Reset all state variables to ensure a fresh start.\n",
        "2. **Set Language and Level**: Set the user's native language, target language, and proficiency level.\n",
        "3. **Split Text into Paragraphs**: Divide the input text into separate paragraphs for step-by-step processing.\n",
        "4. **Process First Paragraph**: Call `process_next_paragraph` to start processing the first paragraph.\n"
      ],
      "metadata": {
        "id": "6BuShd8wrXQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@profile_func\n",
        "def start_processing(native_language, target_language, level, text):\n",
        "    initialize_variables()\n",
        "    state['native_language'] = native_language\n",
        "    state['target_language'] = target_language\n",
        "    state['level'] = level\n",
        "\n",
        "    state['paragraphs'] = text.strip().split('\\n')\n",
        "    state['current_paragraph_index'] = 0\n",
        "\n",
        "    return process_next_paragraph([])"
      ],
      "metadata": {
        "id": "7vwvDWRjr2Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Next Paragraph Function\n",
        "\n",
        "The `process_next_paragraph` function processes the next paragraph of the text, updating the app state as it goes. This function is called repeatedly to process each paragraph in the input text.\n",
        "\n",
        "Key steps:\n",
        "1. **Check Paragraph Index**: If there are more paragraphs to process, it proceeds with the next one.\n",
        "2. **Process Paragraph**: Calls the `process_paragraph` function to handle the current paragraph.\n",
        "3. **Update State**: Updates the state variables with the processed data.\n",
        "4. **Display Output**: Formats and returns the processed paragraph with highlights and translations.\n",
        "5. **Generate Summary**: If all paragraphs are processed, it generates a summary of all unknown words.\n"
      ],
      "metadata": {
        "id": "-PCH9GdLr24C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_next_paragraph(input_unknown_words):\n",
        "    global state\n",
        "    if state['current_paragraph_index'] < len(state['paragraphs']):\n",
        "        paragraph = state['paragraphs'][state['current_paragraph_index']].strip()\n",
        "        if not paragraph:\n",
        "            state['current_paragraph_index'] += 1\n",
        "            return process_next_paragraph(input_unknown_words)\n",
        "        sentences, translated_sentences, final_unknown_words, validated_translations, cognate_pairs = process_paragraph(\n",
        "            [paragraph],\n",
        "            input_unknown_words,\n",
        "            state['known_words'],\n",
        "            state['unknown_words'],\n",
        "            state['validated_translations'],\n",
        "            state['word_count']\n",
        "        )\n",
        "        state['all_final_unknown_words'].extend(final_unknown_words)\n",
        "        state['all_cognate_pairs'].update(cognate_pairs)\n",
        "        output = display_output(paragraph, final_unknown_words)\n",
        "        state['current_paragraph_index'] += 1\n",
        "        print(f\"Paragraph {state['current_paragraph_index']} processed with input unknown words: {input_unknown_words}\")  # Debugging: Print paragraph processing status\n",
        "        return output\n",
        "    else:\n",
        "        summary = generate_summary()\n",
        "        return f\"All paragraphs processed<br>{summary}\""
      ],
      "metadata": {
        "id": "F6tL8Wn-sHbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Highlighted Paragraph Function\n",
        "\n",
        "The `highlighted_paragraph` function highlights unknown words in a paragraph by wrapping them in HTML tags and adding translations.\n",
        "\n",
        "Key steps:\n",
        "1. **Preserve Case**: The function preserves the original casing of the words when replacing them with highlighted versions.\n",
        "2. **Highlight Words**: Each unknown word is wrapped in `<b>` tags and its translation is appended in parentheses.\n",
        "3. **Return Highlighted Paragraph**: Returns the formatted paragraph with highlighted unknown words and translations.\n"
      ],
      "metadata": {
        "id": "wEFHMNfAsSUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def highlighted_paragraph(paragraph, final_unknown_words, validated_translations):\n",
        "    def preserve_case_replace(match, replacement):\n",
        "        matched_text = match.group()\n",
        "        if matched_text.isupper():\n",
        "            return replacement.upper()\n",
        "        elif matched_text[0].isupper():\n",
        "            return replacement.capitalize()\n",
        "        else:\n",
        "            return replacement\n",
        "\n",
        "    highlighted_paragraph = paragraph\n",
        "    for word in final_unknown_words:\n",
        "        original_word = state['original_word_mapping'].get(word['text'], word['text'])\n",
        "        translation_info = next((item for item in validated_translations if item['original'] == word['text']), None)\n",
        "        if translation_info:\n",
        "            translation = translation_info['translation']\n",
        "            highlighted_paragraph = re.sub(r'\\b{}\\b'.format(re.escape(original_word)),\n",
        "                                           lambda match: preserve_case_replace(match, f\"<b>{original_word}</b>({translation})\"),\n",
        "                                           highlighted_paragraph, flags=re.IGNORECASE)\n",
        "    return highlighted_paragraph"
      ],
      "metadata": {
        "id": "CmPZHvbSsbVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Output Function\n",
        "\n",
        "This function, `display_output`, formats the processed paragraph and unknown words for display. It highlights unknown words in the paragraph and provides contextual sentences to help learners understand the usage of these words.\n",
        "\n",
        "Key steps:\n",
        "1. **Highlight Paragraph**: Calls `highlighted_paragraph` to get the formatted paragraph with highlighted unknown words.\n",
        "2. **Generate Contextual Sentences**: Creates sentences showing each unknown word in its context.\n",
        "3. **Format Output**: Combines the highlighted paragraph and contextual sentences into HTML format for display."
      ],
      "metadata": {
        "id": "VTkBVYazscVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_output(paragraph, final_unknown_words):\n",
        "    highlighted_para = highlighted_paragraph(paragraph, final_unknown_words, state['validated_translations'])\n",
        "    context_sentences = []\n",
        "    for word in final_unknown_words:\n",
        "        translation = word.get('translation', 'No translation available')\n",
        "        context_sentence = f\"<b>{word['text']}:</b> <b>{translation}</b>.<br>{word.get('translated_sentence', 'No sentence available')}<br>\"\n",
        "        context_sentences.append(context_sentence)\n",
        "\n",
        "    context_output = \"<br>\".join(context_sentences)\n",
        "    original_paragraphs = paragraph.split(' ')\n",
        "    highlighted_original_para = highlighted_paragraph(\" \".join(original_paragraphs), final_unknown_words, state['validated_translations'])\n",
        "\n",
        "    return f\"<p><b style='font-size: larger;'>Highlighted Text:</b></p><p>{highlighted_original_para}</p><hr><p><b style='font-size: larger;'>Predicted Unknown Words In Context:</b></p><p>{context_output}</p>\""
      ],
      "metadata": {
        "id": "x2eHVqE9sqBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Summary Function\n",
        "\n",
        "The `generate_summary` function compiles a summary of all unknown words encountered during text processing. It provides the count of appearances for each unknown word and its translation, helping learners review new vocabulary.\n",
        "\n",
        "Key steps:\n",
        "1. **Initialize Summary**: Starts with a heading for the summary.\n",
        "2. **Compile Translations**: Gathers translations for all unknown words from the validated translations.\n",
        "3. **Format Summary**: Creates a summary listing each unknown word, its appearance count, and translation.\n"
      ],
      "metadata": {
        "id": "Wct3lElSssgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary():\n",
        "    summary = \"<p style='font-size: larger;'><b>Summary of Unknown Words:</b></p><br>\"\n",
        "\n",
        "    translations_dict = {word: next((item for item in state['validated_translations'] if item['original'] == word), {}).get('translation', 'No translation found')\n",
        "                         for word in state['final_unknown_word_counts'].keys()}\n",
        "\n",
        "    for word, count in state['final_unknown_word_counts'].items():\n",
        "        translation = translations_dict.get(word, 'No translation found')\n",
        "        summary += f\"<b>{word}:</b> {count} appearances, Translation: {translation}<br>\"\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "ZRQ8-MQYs2uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Paragraph Function\n",
        "\n",
        "This function, `next_paragraph`, is used to process the next paragraph in the text based on additional unknown words input by the user. It calls `process_next_paragraph` with the new unknown words and returns the output for the next paragraph.\n",
        "\n",
        "Key steps:\n",
        "1. **Convert Input to List**: Converts the input string of unknown words into a list.\n",
        "2. **Process Next Paragraph**: Calls `process_next_paragraph` with the list of input unknown words."
      ],
      "metadata": {
        "id": "45P_SgtOs8Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_paragraph(input_unknown_words):\n",
        "    if isinstance(input_unknown_words, str):\n",
        "        input_unknown_words = input_unknown_words.split()\n",
        "    return process_next_paragraph(input_unknown_words)"
      ],
      "metadata": {
        "id": "ehUu4tiAs9gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reset Interface Function\n",
        "\n",
        "The `reset_interface` function resets the entire interface and state variables, allowing the user to start over with a new text. This function is useful when the user wants to restart the session.\n",
        "\n",
        "Key steps:\n",
        "1. **Initialize Variables**: Calls `initialize_variables` to reset all state variables.\n",
        "2. **Update Interface**: Resets the Gradio interface elements to their initial state."
      ],
      "metadata": {
        "id": "-tR5msQStFdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_interface():\n",
        "    initialize_variables()\n",
        "    return gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value='')"
      ],
      "metadata": {
        "id": "YclfNWMPtUm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Interface\n",
        "\n",
        "This section defines the Gradio interface for the language learning app. The interface includes inputs for the native language, target language, proficiency level, and text. It also provides buttons to start processing, move to the next paragraph, and restart the app.\n",
        "\n",
        "Key components:\n",
        "1. **Dropdowns**: For selecting native language, target language, and proficiency level.\n",
        "2. **Textbox**: For entering the text to be processed.\n",
        "3. **Buttons**: For starting the processing, moving to the next paragraph, and restarting the app.\n",
        "4. **Output Area**: Displays the processed text with highlighted unknown words and translations."
      ],
      "metadata": {
        "id": "-LoWO6MmtiCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface\n",
        "iface = gr.Blocks()\n",
        "\n",
        "with iface:\n",
        "    native_language_input = gr.Dropdown(choices=['en'], label='Native Language', value='en')\n",
        "    target_language_input = gr.Dropdown(choices=['es'], label='Target Language')\n",
        "    level_input = gr.Dropdown(choices=['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], label='Level')\n",
        "    text_input = gr.Textbox(label='Text', lines=10)\n",
        "    start_button = gr.Button('Start')\n",
        "    output_area = gr.HTML()\n",
        "    unknown_words_input = gr.Textbox(label='Input Unknown Words', lines=2)\n",
        "    next_button = gr.Button('Next Paragraph')\n",
        "    restart_button = gr.Button('Restart')\n",
        "\n",
        "    start_button.click(start_processing, [native_language_input, target_language_input, level_input, text_input], [output_area])\n",
        "    next_button.click(next_paragraph, [unknown_words_input], [output_area])\n",
        "    restart_button.click(reset_interface, [], [native_language_input, target_language_input, level_input, text_input, output_area, unknown_words_input])\n",
        "\n",
        "iface.launch(share=True, debug=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "0GufNasUbOri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentation and Tuning\n",
        "\n",
        "These are the parts of the code can be easily modified to experiment with different results and potentially improve the performance or accuracy of the prototype:\n",
        "\n",
        "### Fuzz Threshold in Cognate Identification\n",
        "\n",
        "In the `find_cognates` function, the `similarity_threshold` can be adjusted to see if it helps in better identifying cognates. The `similarity_threshold` determines how similar two words need to be to be considered cognates. This threshold ranges from 0 to 1, where a higher value means that the words need to be more similar to be identified as cognates. Setting a high threshold, such as 0.8 or 0.9, makes it stricter, so only very similar words are identified as cognates. This reduces false positives but might miss some valid cognates. Conversely, a lower threshold, such as 0.5 or 0.6, makes it more lenient, allowing more words to be identified as cognates, which can include false positives.\n",
        "\n",
        "### Frequency Thresholds for Different Levels\n",
        "\n",
        "The `frequency_thresholds` dictionary defines thresholds for different proficiency levels, determining which words are considered known or unknown based on their frequency in the language. These thresholds can be adjusted to see how it affects the identification of unknown words. For instance, lowering the threshold for the A1 level means that more words will be considered unknown, which might be less overwhelming for beginners. On the other hand, increasing the threshold will consider more words as known, potentially making the reading experience more challenging.\n",
        "\n",
        "### Translation Service\n",
        "\n",
        "The `safe_translate` function uses GoogleTranslator to translate sentences. Using a different translation service like YandexTranslator or DeepL might offer different levels of accuracy and performance, but each service has its limitations, such as API call limits, costs, or different degrees of language support.\n",
        "\n",
        "### Similarity Scoring Method\n",
        "\n",
        "The `rapidfuzz` library is used for similarity scoring in the `validate_translation_in_context` function. You can experiment with different similarity scorers like `fuzz.token_sort_ratio` or `fuzz.partial_ratio`, and adjust the scoring cutoff values to see if it improves translation validation. Higher cutoff values make the matching criteria stricter, which can reduce false positives but might miss valid matches. Lowering the cutoff values can include more matches but at the risk of increased false positives. Changing the scoring method and cutoff values helps find the optimal balance for accurate translation validation.\n",
        "\n",
        "### Batch Size for Preprocessing\n",
        "\n",
        "The `batch_preprocess_text` function processes text in batches. Adjusting the batch size, such as processing paragraphs in smaller or larger batches, can impact efficiency and performance."
      ],
      "metadata": {
        "id": "8oCxbIrpxzJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the prototype\n",
        "Here are some ideas on how to measure the accuracy and effectiveness of the predictions:\n",
        "### 1. Ground Truth Data\n",
        "The first and easiest approach, which can be done without having users to test the app, is to use ground truth data, where we get or create pre-annotated texts with known unknown words for different proficiency levels. We can then compare the app's predictions with these annotations. Data for expected known and unknown words according to the proficiency level can be gathered form CEFR lexical sets. Another good place to look for data is [CL Anthology](https://aclanthology.org/search/?q=spanish+unknown+words), a repository of research papers in computational linguistics that often include datasets.\n",
        "#### Precision and Recall:\n",
        "Calculate precision and recall for the predicted unknown words compared to the ground truth annotations. Precision measures the proportion of correctly identified unknown words out of all predicted unknown words, while recall measures the proportion of correctly identified unknown words out of all actual unknown words.\n",
        "#### F1 Score:\n",
        "Compute the F1 score, which is the harmonic mean of precision and recall, to provide a single metric for evaluation.\n",
        "### 2. User Feedback\n",
        "When user feedback becomes available, we can ask users to mark which predicted unknown words they actually found challenging and measure the percentage of user agreement with the app's predictions.\n",
        "### 3. Weighted Precision and Recall\n",
        "For a more comprehensive evaluation, we should note that not all errors should be treated with the same importance. The severity of an error in the context of this language learning app can vary based on the type of word and its role in the sentence or text.\n",
        "To reflect the different severities of errors, we can use weighted precision and recall. Here, different types of errors are assigned different weights based on their importance.\n",
        "#### Error Severity\n",
        "Here are some examples of how we can classify the severity of the errors based on their impact on understanding the text:\n",
        "*   **False Positives (Unnecessary Translations)**: Less severe. These are words that the app translates but the user didn't need help with. They might create noise in the interface but they will not affect understanding.\n",
        "*   **False Negatives (Missed Translations)**: More severe. These are words that the app didn't translate but the user needed help with. They can significantly affect understanding.\n",
        "*   **Key Content Words**: Very severe if missed. These include nouns, verbs, adjectives, and adverbs that are crucial for understanding the sentence.\n",
        "*   **Function Words**: Less severe if missed. These include conjunctions, prepositions, articles, etc., that add grammatical structure but less semantic meaning.\n",
        "\n",
        "#### Importance-Based Weighting\n",
        "To implement the above, we can assign weights to different types of words and errors. For example:\n",
        "*   **False Negatives (Key Content Words)**: Weight = 3\n",
        "*   **False Negatives (Function Words)**: Weight = 2\n",
        "*   **False Positives (Key Content Words)**: Weight = 1\n",
        "*   **False Positives (Function Words)**: Weight = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ri4xPST10gKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expanding to different languages\n",
        "To adapt the code for different language pairs we need to consider the a series of modifications. This involves updating language-specific components such as NLP models, translation services, frequency data, and handling morphological differences.\n",
        "1. NLP Models\n",
        "The current code uses Stanza for tokenization, lemmatization, and part-of-speech tagging. Stanza supports many languages, but not all. If we need to support a language that Stanza does not, we may need to switch to another NLP library such as SpaCy, which also supports a wide range of languages.\n",
        "It is important to ensure the new NLP model provides similar functionality (tokenization, lemmatization, POS tagging, NER). And to consider that the quality of NLP tasks may vary across languages, impacting accuracy.\n",
        "2. Translation Services\n",
        "The code currently uses GoogleTranslator from the deep-translator library. Alternative APIs like DeepL, Yandex, or Microsoft Translator can be considered if a specific language pair is not supported or if a more accurate service is needed for certain languages.\n",
        "Each translation service has different strengths. DeepL is known for high-quality translations but supports fewer languages compared to Google. Costs and API limits also vary.\n",
        "3. Frequency Data\n",
        "The current code uses the wordfreq library for frequency analysis. While it supports many languages, coverage and accuracy might vary. For languages not well-supported by wordfreq, we need to consider alternative frequency data sources or even corpora specific to those languages. Tools like Lexique3 for French or SUBTLEX for various languages can be useful.\n",
        "Frequency data quality is crucial for accurate unknown word prediction, so it is important to ensure that the frequency data is reliable and representative of modern language use.\n",
        "4. Morphological Differences\n",
        "Languages with rich morphology (like Finnish or Turkish) require a more robust morphological analysis. Morfessor or a similar tool can be used for such languages.\n",
        "The current code uses Snowball Stemmer for Spanish. For other languages, stemmers or lemmatizers might vary (for example using the ISRI Stemmer for Arabic).\n",
        "5. Named Entity Recognition (NER)\n",
        "The current code uses Stanza for NER, which supports many languages but its performance could vary depending on the language. If NER is not well-supported for a target language, we need to consider alternatives like SpaCy, Polyglot, or even custom-trained models.\n",
        "6. Cognate Identification\n",
        "Finding Cognates in New Language Pairs\n",
        "Technically CogNet has data for several language pairs, however its quality varies from language to language. Wiktionary or BabelNet can be potential sources for multilingual cognate data.\n",
        "7. User Interface Adjustments\n",
        "We will need to update the Gradio interface to ensure that it supports the input and display of characters from various languages, including those with different scripts (like Arabic).\n",
        "\n",
        "In summary, not all tools support all languages equally. We might need to mix and match different libraries for comprehensive support.\n",
        "These are some possible options for libraries and tools for some of the most used languages as target:\n",
        "1. English\n",
        "NLP: SpaCy, Stanza, NLTK\n",
        "Translation: Google Translator, DeepL, Microsoft Translator\n",
        "Frequency Data: SUBTLEX-US, wordfreq\n",
        "Morphological Analysis: NLTK, SpaCy\n",
        "NER: SpaCy, Stanza\n",
        "2. Mandarin Chinese\n",
        "NLP: Jieba (for tokenization), Stanza, SpaCy (with Chinese models)\n",
        "Translation: Google Translator, Baidu Translate, Microsoft Translator\n",
        "Frequency Data: Chinese Linguistic Data Consortium (CLDC), wordfreq\n",
        "Morphological Analysis: Jieba, Stanza\n",
        "NER: Stanford NLP, SpaCy, Stanza\n",
        "3. French\n",
        "NLP: SpaCy, Stanza, Talismane\n",
        "Translation: Google Translator, DeepL, Microsoft Translator\n",
        "Frequency Data: Lexique3, wordfreq\n",
        "Morphological Analysis: Talismane, SpaCy\n",
        "NER: SpaCy, Stanza\n",
        "4. Arabic\n",
        "NLP: Farasa, MADAMIRA, Stanza\n",
        "Translation: Google Translator, Microsoft Translator\n",
        "Frequency Data: Aralex (Arabic Lexicon Project), wordfreq\n",
        "Morphological Analysis: Farasa, MADAMIRA\n",
        "NER: Farasa, SpaCy (with Arabic models), Stanza\n",
        "5. Russian\n",
        "NLP: SpaCy, Stanza, Natasha\n",
        "Translation: Google Translator, Yandex Translate, Microsoft Translator\n",
        "Frequency Data: ruTenTen (Russian Web Corpus), wordfreq\n",
        "Morphological Analysis: Pymorphy2, SpaCy\n",
        "NER: Natasha, SpaCy, Stanza\n"
      ],
      "metadata": {
        "id": "zr6Neg45arth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful Resources for Identifying Difficult Words for Language Learners\n",
        "\n",
        "While specific datasets for testing the app prototype are not directly available, the following resources can provide valuable insights into word difficulty and vocabulary acquisition for language learners.\n",
        "\n",
        "### 1. Selecting Reading Texts Suitable for Incidental Vocabulary Learning by Considering the Estimated Distribution of Acquired Vocabulary\n",
        "This paper discusses a method for selecting reading texts that support incidental vocabulary learning by considering the distribution of vocabulary already acquired by learners. This approach helps identify which words might be more challenging based on learners' existing vocabulary.\n",
        "- **Link:** [Selecting Reading Texts Suitable for Incidental Vocabulary Learning](https://educationaldatamining.org/EDM2022/proceedings/2022.EDM-posters.99/)\n",
        "\n",
        "### 2. More Than Frequency: Exploring Predictors of Word Difficulty for Second Language Learners\n",
        "This research examines various predictors of word difficulty beyond frequency, including word length, concreteness, and phonological complexity. These insights help predict which words learners are likely to find challenging by considering a range of linguistic factors.\n",
        "- **Link:** [Exploring Predictors of Word Difficulty](https://www.researchgate.net/publication/333144175_More_Than_Frequency_Exploring_Predictors_of_Word_Difficulty_for_Second_Language_Learners)\n",
        "\n",
        "### 3. Harvard Dataverse: Second Language Acquisition Data\n",
        "This dataset includes extensive data on second language acquisition, covering learner interactions and vocabulary tests. Analyzing this dataset provides empirical evidence on common word difficulties and learner patterns, which can inform the apps word prediction and translation algorithms.\n",
        "- **Link:** [Second Language Acquisition Data](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/8SWHNO)\n"
      ],
      "metadata": {
        "id": "7TaQN2sRz6Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k-wvH7MUzEVH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}